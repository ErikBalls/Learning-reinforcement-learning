{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04a689a0-3a4b-4c94-82e6-75db8ee34514",
   "metadata": {},
   "source": [
    "# Required installations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f4b869-aad0-4315-94c3-0da007b1744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch\n",
    "#!pip install tensorflow\n",
    "#!pip install keras\n",
    "#!pip install keras-rl2\n",
    "#!pip install gym\n",
    "#!pip install matplotlib\n",
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install stable_baselines3[extra]\n",
    "#!pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d072cf-ae20-4c31-876d-5d8e0d794324",
   "metadata": {},
   "source": [
    "# Important imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6661bdf4-51cf-4bf3-9bd0-a18d8edcf21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA ist verfügbar.\n",
      "Aktuelles Gerät: 0\n",
      "Gerätename: NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DDPG, TD3, A2C, PPO\n",
    "from stable_baselines3.common.noise import NormalActionNoise #importing Gaussian Noise\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Überprüfen, ob CUDA verfügbar ist\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA ist verfügbar.')\n",
    "    print('Aktuelles Gerät:', torch.cuda.current_device())\n",
    "    print('Gerätename:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print('CUDA ist nicht verfügbar.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838977a9-d5b9-4786-9031-676269a0727f",
   "metadata": {},
   "source": [
    "# Initializising environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6212b03e-f864-4688-acfe-0bd9526be97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v1', g = 9.81, render_mode=\"rgb_array\") #choose gym from https://gymnasium.farama.org/. Choose render mode \"rgb_array\" or choose \"human\" to visiualize the game but it will take more time until the action is done\n",
    "env.reset() #clear the environment\n",
    "\n",
    "print(\"sample action\", env.action_space.sample()) #Value for action\n",
    "print(\"observation space shape\", env.observation_space.shape) #Tensor for possible states (In pendulum there are 3 states of the pendulum: x-,y coordinate and angular velocity)\n",
    "print(\"sample observation\", env.observation_space.sample()) #Actual values inside the Tensor (Since there are 3 states of the pendulum, the tensor includes 3 values of the states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37958a9-83b6-4885-95ab-e518cf1f3719",
   "metadata": {},
   "source": [
    "# Let the game be played without any sense \n",
    "## (Don't forget \"human\" Render Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb8306c-9d9a-4cc5-84ee-f4d1af9888f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10 #initialise for loop\n",
    "for episode in range(1, episodes + 1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done: #what to do in the for loop\n",
    "        env.render() #render the process\n",
    "        action = env.action_space.sample() #what action can be taken\n",
    "        n_state, reward, terminated, truncated, info = env.step(action) #return of the parameters\n",
    "        score += reward #sum of the reward\n",
    "        done = terminated or truncated\n",
    "        \n",
    "    print(\"Episode:{} Score:{}\".format(episode, score)) #print the number and the reward of the episode\n",
    "    print(\"sample action\", env.action_space.sample()) #Value for action\n",
    "    print(\"sample observation\", env.observation_space.sample()) #Actual values inside the Tensor (Since there are 3 states of the pendulum, the tensor includes 3 values of the states)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edc3494-d144-446d-921d-fd53eabb7296",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e095baba-55dc-4bbe-a8b2-85639bd134c9",
   "metadata": {},
   "source": [
    "# Building, training and saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab43be-7b24-4512-8f14-4e9a4510c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"models/Test\" #making a Folder in your explorer with the data , dont forget to name your folder according to your model name\n",
    "logdir = \"logs\" \n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "    \n",
    "env.reset()\n",
    "\n",
    "n_actions = env.action_space.shape[0]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "model = DDPG(\"MlpPolicy\", env, verbose = 1, tensorboard_log=logdir, action_noise=action_noise) #defining model as DDPG/TD3/A2C/PPO Model with given policy, environment, verbose = 1 for returning infos, sending infos to tensorboard and the noise for better exploration\n",
    "#action_noise=action_noise cannot be applied to stochastic models like A2C or PPO!\n",
    "TIMESTEPS = 10000 #defining when a model will be saved, in this case every 10000 steps\n",
    "iters = 0\n",
    "for i in range(20): #for 20 times 10000 = 200000 steps the model will learn.\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=\"Test\") #change the tb_log_name to create different folders in your explorer\n",
    "    model.save(f\"{models_dir}/{TIMESTEPS*i}\")\n",
    "#to see data in your tensorboard, open a terminal in your corresponding directory and type \"tensorboard --logdir=logs\" while the model is learning\n",
    "#to load the data when you closed tensorboard type \"tensorboard --logdir path/to/logs\"\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22639143-018f-46e6-9cd6-f1cdc081c1bb",
   "metadata": {},
   "source": [
    "# Load the trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b095c-9a84-4c33-8403-926129bd364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"models/DDPG\"\n",
    "model_path = f\"{models_dir}/180000.zip\"\n",
    "logdir = \"logs\" \n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "    \n",
    "env = gym.make('Pendulum-v1', g = 9.81, render_mode=\"human\") \n",
    "env.reset() \n",
    "\n",
    "\n",
    "\n",
    "model = DDPG.load(model_path, env = env)\n",
    "print(\"observation space shape\", env.observation_space.shape)\n",
    "episodes = 10\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs = env.observation_space.sample()\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a90c84f-91b6-4f69-8630-a846fefe392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b845e748-e7a3-43e3-bd20-770b65f1b56f",
   "metadata": {},
   "source": [
    "# Evaluation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333425c0-ed21-466f-a448-4ecc0b6c6ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "df_DDPG = pd.read_csv('logs_DDPG_w_o_noise_03.09.24_0.csv') #define your data logs here\n",
    "df_TD3 = pd.read_csv('logs_TD3_w_o_noise_03.09.24_0.csv')\n",
    "df_A2C = pd.read_csv('logs_A2C_03.09.24_0.csv')\n",
    "df_PPO = pd.read_csv('logs_PPO_03.09.24_0.csv')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "plt.scatter(df_DDPG.Step, df_DDPG.Value, marker = '.', color = 'red', s = 10, label='DDPG') #DIY your own diagram\n",
    "plt.scatter(df_TD3.Step, df_TD3.Value, marker = '.', color = 'blue', s = 10, label='TD3')\n",
    "plt.scatter(df_A2C.Step, df_A2C.Value, marker = '.', color = 'gray', s = 10, label='A2C')\n",
    "plt.scatter(df_PPO.Step, df_PPO.Value, marker = '.', color = 'black', s = 10, label='PPO')\n",
    "\n",
    "plt.title('Comparison of the models in the Pendelum environment', loc = 'left')\n",
    "\n",
    "max_value_DDPG = df_DDPG.Value.max() #defining max values of the data logs to compare the models\n",
    "max_step_DDPG = df_DDPG[df_DDPG.Value == max_value_DDPG].Step.values[0]\n",
    "max_value_TD3 = df_TD3.Value.max()\n",
    "max_step_TD3 = df_TD3[df_TD3.Value == max_value_TD3].Step.values[0]\n",
    "plt.scatter(max_step_DDPG, max_value_DDPG, color='red', s=30, marker='o')\n",
    "plt.annotate(f'Max: {max_value_DDPG:.2f}', (max_step_DDPG, max_value_DDPG), xytext=(125000, max_value_DDPG + -120), color = 'blue')\n",
    "plt.scatter(max_step_TD3, max_value_TD3, color='blue', s=30, marker='o')\n",
    "plt.annotate(f'Max: {max_value_TD3:.2f}', (max_step_TD3, max_value_TD3), xytext=(160000, max_value_DDPG + -120), color = 'red')\n",
    "#plt.annotate(f'Max: {max_step_TD3:.2f}', (max_step_TD3, max_value_TD3)) #check step when max reward was recieved\n",
    "#plt.annotate(f'Max: {max_step_DDPG:.2f}', (max_step_DDPG, max_value_DDPG)) #check step when max reward was recieved\n",
    "plt.axvline(x=max_step_DDPG, color='gray', linestyle='--', alpha = 0.5, dashes=(5, 10))\n",
    "plt.axvline(x=max_step_TD3, color='gray', linestyle='--', alpha = 0.5, dashes=(5, 10))\n",
    "\n",
    "\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Reward')\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(-0.2, 0.91))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace7352c-3044-430f-bf8c-36b351c054d2",
   "metadata": {},
   "source": [
    "#### We are going to compare the different trained models in the Pendulum environment. We can see the recieved reward of the models on the y-axis and the amount of steps taken on the x-axis. The most rewards recieved the TD3 model on approximatly 154000 steps. The second most rewards recieved the DDPG model on approximatly 189000 steps. \n",
    "#### Overall the DDPG and the TD3 model learned the Pendulum Environment pretty fast compared to the A2C and the PPO models. Thanks to the actor-critic-architecture of the DDPG and TD3 the agents can remember previous rewards of previous steps and learn from the past. The A2C and PPO agents have no memory by default. That's why the DDPG and TD3 agents learn faster than the A2C and PPO agents.\n",
    "#### The learning curve of the DDPG and TD3 model are similiar to each over but the learning curve of the DDPG model reached the convergence region faster than the TD3 model. This is because of the delayed critic network of the TD3 Agent (Or Twin Delayed DDPG Agent). In order to prevent greedy decisions of the actor the critic network updates itself less often than the DDPG critic. This can be a advantage in some cases but in this case this delayed the learning curve from reaching the convergence region.\n",
    "#### Summa summarum the DDPG and the TD3 model learned the environment fast and managed to solve the task. The A2C and PPO models have to be trained more steps in order to learn what to do with the pendulum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87693786-098a-47a4-91a4-29f8ad492f38",
   "metadata": {},
   "source": [
    "# Using a custom environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe59ca73-0452-48f7-8bfc-8af6c537a8d4",
   "metadata": {},
   "source": [
    "#### Let's explore a custom environment for our next challenge. We'll simulate a Permanent Magnet Synchronous Motor (PMSM) and train two agents, DDPG and TD3, to control the motor's current, with the objective of maximizing torque output. Thankfully clever people have already developed a custom environment for this simulation. In this section, we'll follow the approach outlined on the Gym Electric Motor (GEM) GitHub page. For more detailed information, visit: https://github.com/upb-lea/gym-electric-motor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ffd16-a665-4da5-8c52-99f7edca1bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym-electric-motor\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c4657c-acf3-45c9-9535-d75a8145960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_electric_motor as gem\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from gym_electric_motor.reference_generators import LaplaceProcessReferenceGenerator\n",
    "from gym_electric_motor.visualization import MotorDashboard\n",
    "from gym_electric_motor.core import Callback\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "from gymnasium.wrappers import FlattenObservation, TimeLimit\n",
    "from gymnasium import ObservationWrapper\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba7658f-3f6d-49de-ba9d-4ee9c72499d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardLogger(Callback):\n",
    "    \"\"\"Logs the reward accumulated in each episode\"\"\"\n",
    "    def __init__(self):\n",
    "        self.step_rewards = []\n",
    "        self.mean_episode_rewards = []\n",
    "        dir_path = Path.cwd() / \"PMSM_simulation\" / \"saved_agents\"\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.fpath = dir_path  / \"EpisodeRewards.npy\"\n",
    "        \n",
    "    def on_step_end(self, k, state, reference, reward, done):\n",
    "        \"\"\"Stores the received reward at each step\"\"\"\n",
    "        self.step_rewards.append(reward)\n",
    "    \n",
    "    def on_reset_begin(self):\n",
    "        \"\"\"Stores the mean reward received in every episode\"\"\"\n",
    "        if len(self.step_rewards) > 0:\n",
    "            self.mean_episode_rewards.append(np.mean(self.step_rewards))\n",
    "        self.step_rewards = []\n",
    "        \n",
    "    def on_close(self):\n",
    "        \"\"\"Writes the mean episode reward of the experiment to a file.\"\"\"\n",
    "        np.save(self.fpath, np.array(self.mean_episode_rewards))\n",
    "\n",
    "class FeatureWrapper(ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper class which wraps the environment to change its observation. Serves\n",
    "    the purpose to improve the agent's learning speed.\n",
    "    \n",
    "    It changes epsilon to cos(epsilon) and sin(epsilon). This serves the purpose\n",
    "    to have the angles -pi and pi close to each other numerically without losing\n",
    "    any information on the angle.\n",
    "    \n",
    "    Additionally, this wrapper adds a new observation i_sd**2 + i_sq**2. This should\n",
    "    help the agent to easier detect incoming limit violations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, epsilon_idx, i_sd_idx, i_sq_idx):\n",
    "        \"\"\"\n",
    "        Changes the observation space to fit the new features\n",
    "        \n",
    "        Args:\n",
    "            env(GEM env): GEM environment to wrap\n",
    "            epsilon_idx(integer): Epsilon's index in the observation array\n",
    "            i_sd_idx(integer): I_sd's index in the observation array\n",
    "            i_sq_idx(integer): I_sq's index in the observation array\n",
    "        \"\"\"\n",
    "        super(FeatureWrapper, self).__init__(env)\n",
    "        self.EPSILON_IDX = epsilon_idx\n",
    "        self.I_SQ_IDX = i_sq_idx\n",
    "        self.I_SD_IDX = i_sd_idx\n",
    "        new_low = np.concatenate((self.env.observation_space.low[     \n",
    "                                  :self.EPSILON_IDX], np.array([-1.]),\n",
    "                                  self.env.observation_space.low[\n",
    "                                  self.EPSILON_IDX:], np.array([0.])))\n",
    "        new_high = np.concatenate((self.env.observation_space.high[\n",
    "                                   :self.EPSILON_IDX], np.array([1.]),\n",
    "                                   self.env.observation_space.high[\n",
    "                                   self.EPSILON_IDX:],np.array([1.])))\n",
    "\n",
    "        self.observation_space = Box(new_low, new_high)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        \"\"\"\n",
    "        Gets called at each return of an observation. Adds the new features to the\n",
    "        observation and removes original epsilon.\n",
    "        \n",
    "        \"\"\"\n",
    "        cos_eps = np.cos(observation[self.EPSILON_IDX] * np.pi)\n",
    "        sin_eps = np.sin(observation[self.EPSILON_IDX] * np.pi)\n",
    "        currents_squared = observation[self.I_SQ_IDX]**2 + observation[self.I_SD_IDX]**2\n",
    "        observation = np.concatenate((observation[:self.EPSILON_IDX],\n",
    "                                      np.array([cos_eps, sin_eps]),\n",
    "                                      observation[self.EPSILON_IDX + 1:],\n",
    "                                      np.array([currents_squared])))\n",
    "        return observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68376181-eecd-4ea9-bbe6-e1b94035b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define motor arguments\n",
    "motor_parameter = dict(\n",
    "    p=3,  # [p] = 1, nb of pole pairs\n",
    "    r_s=17.932e-3,  # [r_s] = Ohm, stator resistance\n",
    "    l_d=0.37e-3,  # [l_d] = H, d-axis inductance\n",
    "    l_q=1.2e-3,  # [l_q] = H, q-axis inductance\n",
    "    psi_p=65.65e-3,  # [psi_p] = Vs, magnetic flux of the permanent magnet\n",
    ")\n",
    "# supply voltage\n",
    "u_supply = 400\n",
    "\n",
    "# Replace the reference generator by passing a new instance\n",
    "my_new_ref_gen_instance = LaplaceProcessReferenceGenerator(\n",
    "    reference_state='i_sq',\n",
    "    sigma_range=(1e-3, 1e-2)\n",
    ")\n",
    "\n",
    "# nominal and absolute state limitations\n",
    "nominal_values=dict(\n",
    "    omega=418.879, #4000*2*np.pi/60\n",
    "    i=230,\n",
    "    u=400 #u_supply\n",
    ")\n",
    "limit_values=dict(\n",
    "    omega=418.879,\n",
    "    i=345,\n",
    "    u=400 #u_supply\n",
    ")\n",
    "\n",
    "# sampling interval\n",
    "tau = 1e-5\n",
    "\n",
    "# define maximal episode steps\n",
    "max_eps_steps = 10000\n",
    "\n",
    "motor_initializer = {'random_init': 'uniform', 'interval': np.array([[-230, 230], [-230, 230], [-np.pi, np.pi]])}\n",
    "reward_function=gem.reward_functions.WeightedSumOfErrors(\n",
    "    reward_weights={'i_sq': 10, 'i_sd': 10},\n",
    "    gamma=0.99,  # discount rate \n",
    "    reward_power=1\n",
    ")\n",
    "reward_logger = RewardLogger()\n",
    "motor_dashboard = MotorDashboard(state_plots=['i_sq', 'i_sd'], reward_plot=True)\n",
    "# creating gem environment\n",
    "env = gem.make(  # define a PMSM with continous action space\n",
    "    'Cont-CC-PMSM-v0',\n",
    "    # visualize the results\n",
    "    visualization=motor_dashboard,\n",
    "    voltage_supply=u_supply,\n",
    "    reference_generator=my_new_ref_gen_instance,\n",
    "    \n",
    "    # parameterize the PMSM and update limitations\n",
    "    motor=dict(\n",
    "        motor_parameter=motor_parameter,\n",
    "        limit_values=limit_values,\n",
    "        nominal_values=nominal_values,\n",
    "        motor_initializer=motor_initializer,\n",
    "    ),\n",
    "    # define the random initialisation for load and motor\n",
    "    load=dict(\n",
    "        load_initializer={'random_init': 'uniform', },\n",
    "    ),\n",
    "    reward_function=reward_function,\n",
    "    supply=dict(u_nominal=u_supply),\n",
    "    # define the duration of one sampling step\n",
    "    tau=tau,\n",
    "    callbacks=(reward_logger,),\n",
    "    ode_solver='euler',\n",
    ")\n",
    "\n",
    "env.action_space = Box(low=np.array([-345.0, -345.0, -345.0]), high=np.array([345.0, 345.0, 345.0]), dtype=np.float32)\n",
    "\n",
    "# applying wrappers\n",
    "eps_idx = env.physical_system.state_names.index('epsilon')\n",
    "i_sd_idx = env.physical_system.state_names.index('i_sd')\n",
    "i_sq_idx = env.physical_system.state_names.index('i_sq')\n",
    "env = TimeLimit(\n",
    "    FeatureWrapper(\n",
    "        FlattenObservation(env), \n",
    "        eps_idx, i_sd_idx, i_sq_idx\n",
    "    ),\n",
    "    max_eps_steps\n",
    ")\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bfedf3-b9b2-47f3-80fa-38b9bd7013f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 200000 #number of old obsersation steps saved\n",
    "learning_starts = 10000 # memory warmup\n",
    "train_freq = 1 # prediction network gets an update each train_freq's step\n",
    "batch_size = 25 # mini batch size drawn at each update step\n",
    "policy_kwargs = {\n",
    "        'net_arch': [64,64] # hidden layer size of MLP\n",
    "        }\n",
    "exploration_fraction = 0.1 # Fraction of training steps the epsilon decays \n",
    "target_update_interval = 1000 # Target network gets updated each target_update_interval's step\n",
    "gamma = 0.99\n",
    "verbose = 1 # verbosity of stable-basline's prints\n",
    "tau = 1e-5\n",
    "simulation_time = 5 # seconds\n",
    "nb_steps = int(simulation_time // tau)\n",
    "\n",
    "# Überprüfen, ob CUDA verfügbar ist\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA ist verfügbar.')\n",
    "    print('Aktuelles Gerät:', torch.cuda.current_device())\n",
    "    print('Gerätename:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print('CUDA ist nicht verfügbar.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47acc6fe-46ed-4fa0-8db7-5f02c58b188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"PMSM_simulation/high_ampere/DDPG\" #name your directory\n",
    "logdir = \"PMSM_simulation/logs\"\n",
    "\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "total_timesteps = 50000\n",
    "\n",
    "model = DDPG(\"MlpPolicy\", env, buffer_size=buffer_size, learning_starts=learning_starts ,train_freq=train_freq,#you can decide between TD3 and DDPG Agent\n",
    "            batch_size=batch_size, gamma=gamma, policy_kwargs=policy_kwargs, \n",
    "            verbose=verbose, tensorboard_log=logdir)\n",
    "\n",
    "for i in range(20): #20 times -> 10mio steps overall\n",
    "    model.learn(total_timesteps=nb_steps, reset_num_timesteps=False, tb_log_name=\"DDPG_PMSM_23.09.24_high_ampere\")\n",
    "    model.save(f\"{models_dir}/{total_timesteps*i}\")\n",
    "\n",
    "env.close() \n",
    "clear_output() #Model will train for 10mio steps, this took approx. 20h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2178940-6202-4289-bf5d-50f8ba6bd29a",
   "metadata": {},
   "source": [
    "# Evaluating Models with default settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa4def-3c20-44a9-8e7c-2986f0190212",
   "metadata": {},
   "source": [
    "#### We have trained a DDPG and TD3 model for a simulation of a PMSM with a continous action space. The results look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c280f-19c5-4d84-b512-3907dcf1d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "df_DDPG_ep = pd.read_csv('DDPG_PMSM_15.09.24_ep_mean.csv') #define your data logs here\n",
    "df_DDPG_rew = pd.read_csv('DDPG_PMSM_15.09.24_rew_mean.csv')\n",
    "df_TD3_ep = pd.read_csv('TD3_PMSM_16.09.24_ep_mean.csv')\n",
    "df_TD3_rew = pd.read_csv('TD3_PMSM_16.09.24_rew_mean.csv')\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot für DDPG und TD3 auf der ersten y-Achse (Reward)\n",
    "ax1.scatter(df_DDPG_rew.Step, df_DDPG_rew.Value, marker='.', s=10, label='DDPG Reward', color='red')  # DIY your own diagram\n",
    "ax1.scatter(df_TD3_rew.Step, df_TD3_rew.Value, marker='.', s=10, label='TD3 Reward', color='blue')\n",
    "\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.set_title('Comparison of the models in the PMSM environment (continuous action space)', loc='left')\n",
    "\n",
    "# Zweite y-Achse für Episode Length\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(df_DDPG_ep.Step, df_DDPG_ep.Value, color='red',  alpha=0.3, label='DDPG Episode Length')\n",
    "ax2.plot(df_TD3_ep.Step, df_TD3_ep.Value, color='blue',  alpha=0.3, label='TD3 Episode Length')\n",
    "ax2.set_ylabel('Episode Length')\n",
    "\n",
    "# Legende anpassen, um beide Achsen zu berücksichtigen\n",
    "ax1.legend(loc='center left', bbox_to_anchor=(-0.22, 0.9))\n",
    "ax2.legend(loc='center right', bbox_to_anchor=(1.29, 0.88))\n",
    "\n",
    "# Filtern der Daten für x > 0.5e7\n",
    "df_filtered_DDPG = df_DDPG_rew[df_DDPG_rew['Step'] > 0.5e7]\n",
    "df_filtered_TD3 = df_TD3_rew[df_TD3_rew['Step'] > 0.5e7]\n",
    "\n",
    "# Berechnung des maximalen Rewards für DDPG und TD3\n",
    "max_value_DDPG_rew = df_filtered_DDPG.Value.max()\n",
    "max_step_DDPG_rew = df_filtered_DDPG[df_filtered_DDPG.Value == max_value_DDPG_rew].Step.values[0]\n",
    "\n",
    "max_value_TD3_rew = df_filtered_TD3.Value.max()\n",
    "max_step_TD3_rew = df_filtered_TD3[df_filtered_TD3.Value == max_value_TD3_rew].Step.values[0]\n",
    "\n",
    "# Anzeige der Maximalwerte im Diagramm\n",
    "ax1.scatter(max_step_DDPG_rew, max_value_DDPG_rew, color='red', s=30, marker='o')\n",
    "ax1.annotate(f'Max: {max_value_DDPG_rew:.2f}', (max_step_DDPG_rew, max_value_DDPG_rew), \n",
    "             xytext=(0.548e7, max_value_DDPG_rew + 1000), color='red')\n",
    "\n",
    "ax1.scatter(max_step_TD3_rew, max_value_TD3_rew, color='blue', s=30, marker='o')\n",
    "ax1.annotate(f'Max: {max_value_TD3_rew:.2f}', (max_step_TD3_rew, max_value_TD3_rew), \n",
    "             xytext=(0.548e7, max_value_DDPG_rew + 600), color='blue')\n",
    "\n",
    "ax1.axvline(x=max_step_DDPG_rew, color='gray', linestyle='--', alpha = 0.5, dashes=(5, 10))\n",
    "ax1.axvline(x=max_step_TD3_rew, color='gray', linestyle='--', alpha = 0.5, dashes=(5, 10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6836117d-6b3d-499f-a99d-b0cc3eb6e8fe",
   "metadata": {},
   "source": [
    "#### In this analysis, we compare the performance of different trained models in the PMSM (Permanent Magnet Synchronous Motor) environment. The y-axis represents the reward received by the models, while the x-axis shows the number of steps taken during training. The TD3 model achieved the highest reward at approximately 7 million steps, followed by the DDPG model, which reached its peak reward at around 6.1 million steps.\n",
    "#### Both the DDPG and TD3 models encountered challenges in maximizing the reward, where a reward of 0 represents the optimal outcome. During the initial phase (up to 4 million steps), both models were focused on exploring the relationship between episode length and received reward. After this point, the episode length for both models converged to approximately 10,000 steps. Following this convergence, the models' received rewards increased, reaching their respective peaks. Beyond these peaks, a downward trend in reward performance was observed, which may be attributed to the models’ attempt to explore alternative strategies for improving rewards.\n",
    "#### Another notable aspect highlighted in the diagram is the volatility of the DDPG model. Both its reward function and episode length fluctuated significantly, indicating unstable behavior. In contrast, the TD3 model exhibited more stability throughout the training process.\n",
    "#### In summary, both models demonstrated comparable performance. While the DDPG model reached its peak reward and episode length earlier, the TD3 model displayed greater overall stability during the training period.\n",
    "#### The next step will involve fine-tuning the training parameters to potentially enhance the performance of both models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0241c021-9254-489d-8574-be1efcc3e617",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ba8859-eb1c-44a4-85a5-de6dd5e32286",
   "metadata": {},
   "source": [
    "#### The goal of this section is to help our model to achieve better rewards. In order to improve our model, we will change the hyperparameters of the PMSM model. But which parameters should we change and how do we change them? Are we going to raise the parameter values or lower them? Because of the complexity of the PMSM environment, we would have to do trial and error in order to learn what helps our model and what not. That's why we are going to use automated pipelines. Automated pipelines will help us automatically find the best parameter values while the model is training. The disadvantage of automated pipelines is the increase of the training duration of our models. But nethertheless let's see if automated pipelines help our model to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd1f8bd-4474-4911-94cf-14a43ff83106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install optuna\n",
    "#!pip install ipywidgets\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bff1ee4-633f-45b3-a863-29f99bb33c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e6f5519-12ce-4557-8e81-bfb446a73414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile this class definition if you want to tune your hyperparameters. It's possible that you have to restart kernel if you want to train models.\n",
    "class RewardLogger(BaseCallback):\n",
    "    \"\"\"Logs the reward accumulated in each episode\"\"\"\n",
    "\n",
    "    def __init__(self, verbose=0):\n",
    "        super(RewardLogger, self).__init__(verbose)\n",
    "        self.step_rewards = []\n",
    "        self.mean_episode_rewards = []\n",
    "        dir_path = Path.cwd() / \"PMSM_simulation\" / \"saved_agents\"\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.fpath = dir_path / \"EpisodeRewards.npy\"\n",
    "\n",
    "    def set_env(self, env):\n",
    "        \"\"\"Set the environment for the callback.\"\"\"\n",
    "        self.env = env\n",
    "\n",
    "    def on_reset_begin(self):\n",
    "        \"\"\"Stores the mean reward received in every episode\"\"\"\n",
    "        self.step_rewards = []\n",
    "        \n",
    "    def on_close(self):\n",
    "        \"\"\"Writes the mean episode reward of the experiment to a file.\"\"\"\n",
    "        np.save(self.fpath, np.array(self.mean_episode_rewards))\n",
    "\n",
    "    def _on_reset_end(self, *args):\n",
    "        \"\"\"Handles the reset end callback expected by the environment.\"\"\"\n",
    "        pass  # This method is now defined, even if it does nothing.\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"Retrieve the reward for the current step\"\"\"\n",
    "        reward = self.locals['rewards']  # Assuming 'rewards' is accessible\n",
    "        self.step_rewards.append(reward)\n",
    "\n",
    "        # If the episode ends, calculate the mean episode reward\n",
    "        if self.locals['dones']:\n",
    "            episode_mean_reward = np.mean(self.step_rewards)\n",
    "            self.mean_episode_rewards.append(episode_mean_reward)\n",
    "            self.step_rewards = []  # Reset for next episode\n",
    "        np.save(self.fpath, self.mean_episode_rewards)# Save rewards to file\n",
    "        return True  # Keep training\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        \"\"\"Save rewards at the end of training.\"\"\"\n",
    "        np.save(self.fpath, self.mean_episode_rewards)\n",
    "        if self.verbose:\n",
    "            print(\"Training ended and rewards saved.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FeatureWrapper(ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Wrapper class which wraps the environment to change its observation. Serves\n",
    "    the purpose to improve the agent's learning speed.\n",
    "    \n",
    "    It changes epsilon to cos(epsilon) and sin(epsilon). This serves the purpose\n",
    "    to have the angles -pi and pi close to each other numerically without losing\n",
    "    any information on the angle.\n",
    "    \n",
    "    Additionally, this wrapper adds a new observation i_sd**2 + i_sq**2. This should\n",
    "    help the agent to easier detect incoming limit violations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, epsilon_idx, i_sd_idx, i_sq_idx):\n",
    "        \"\"\"\n",
    "        Changes the observation space to fit the new features\n",
    "        \n",
    "        Args:\n",
    "            env(GEM env): GEM environment to wrap\n",
    "            epsilon_idx(integer): Epsilon's index in the observation array\n",
    "            i_sd_idx(integer): I_sd's index in the observation array\n",
    "            i_sq_idx(integer): I_sq's index in the observation array\n",
    "        \"\"\"\n",
    "        super(FeatureWrapper, self).__init__(env)\n",
    "        self.EPSILON_IDX = epsilon_idx\n",
    "        self.I_SQ_IDX = i_sq_idx\n",
    "        self.I_SD_IDX = i_sd_idx\n",
    "        new_low = np.concatenate((self.env.observation_space.low[     \n",
    "                                  :self.EPSILON_IDX], np.array([-1.]),\n",
    "                                  self.env.observation_space.low[\n",
    "                                  self.EPSILON_IDX:], np.array([0.])))\n",
    "        new_high = np.concatenate((self.env.observation_space.high[\n",
    "                                   :self.EPSILON_IDX], np.array([1.]),\n",
    "                                   self.env.observation_space.high[\n",
    "                                   self.EPSILON_IDX:],np.array([1.])))\n",
    "\n",
    "        self.observation_space = Box(new_low, new_high)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        \"\"\"\n",
    "        Gets called at each return of an observation. Adds the new features to the\n",
    "        observation and removes original epsilon.\n",
    "        \n",
    "        \"\"\"\n",
    "        cos_eps = np.cos(observation[self.EPSILON_IDX] * np.pi)\n",
    "        sin_eps = np.sin(observation[self.EPSILON_IDX] * np.pi)\n",
    "        currents_squared = observation[self.I_SQ_IDX]**2 + observation[self.I_SD_IDX]**2\n",
    "        observation = np.concatenate((observation[:self.EPSILON_IDX],\n",
    "                                      np.array([cos_eps, sin_eps]),\n",
    "                                      observation[self.EPSILON_IDX + 1:],\n",
    "                                      np.array([currents_squared])))\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4a8adf0-ccb1-426e-a916-0418198db68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-345.0, 345.0, (3,), float32)\n"
     ]
    }
   ],
   "source": [
    "# define motor arguments\n",
    "motor_parameter = dict(\n",
    "    p=3,  # [p] = 1, nb of pole pairs\n",
    "    r_s=17.932e-3,  # [r_s] = Ohm, stator resistance\n",
    "    l_d=0.37e-3,  # [l_d] = H, d-axis inductance\n",
    "    l_q=1.2e-3,  # [l_q] = H, q-axis inductance\n",
    "    psi_p=65.65e-3,  # [psi_p] = Vs, magnetic flux of the permanent magnet\n",
    ")\n",
    "# supply voltage\n",
    "u_supply = 400\n",
    "\n",
    "# Replace the reference generator by passing a new instance\n",
    "my_new_ref_gen_instance = LaplaceProcessReferenceGenerator(\n",
    "    reference_state='i_sq',\n",
    "    sigma_range=(1e-3, 1e-2)\n",
    ")\n",
    "\n",
    "# nominal and absolute state limitations\n",
    "nominal_values=dict(\n",
    "    omega=418.879, #4000*2*np.pi/60\n",
    "    i=230,\n",
    "    u=400 #u_supply\n",
    ")\n",
    "limit_values=dict(\n",
    "    omega=418.879,\n",
    "    i=345,\n",
    "    u=400 #u_supply\n",
    ")\n",
    "\n",
    "# sampling interval\n",
    "tau = 1e-5\n",
    "\n",
    "# define maximal episode steps\n",
    "max_eps_steps = 10000\n",
    "\n",
    "motor_initializer = {'random_init': 'uniform', 'interval': np.array([[-230, 230], [-230, 230], [-np.pi, np.pi]])}\n",
    "reward_function=gem.reward_functions.WeightedSumOfErrors(\n",
    "    reward_weights={'i_sq': 10, 'i_sd': 10},\n",
    "    gamma=0.99,  # discount rate \n",
    "    reward_power=1\n",
    ")\n",
    "reward_logger = RewardLogger()\n",
    "motor_dashboard = MotorDashboard(state_plots=['i_sq', 'i_sd'], reward_plot=True)\n",
    "# creating gem environment\n",
    "env = gem.make(  # define a PMSM with continous action space\n",
    "    'Cont-CC-PMSM-v0',\n",
    "    # visualize the results\n",
    "    visualization=motor_dashboard,\n",
    "    voltage_supply=u_supply,\n",
    "    reference_generator=my_new_ref_gen_instance,\n",
    "    \n",
    "    # parameterize the PMSM and update limitations\n",
    "    motor=dict(\n",
    "        motor_parameter=motor_parameter,\n",
    "        limit_values=limit_values,\n",
    "        nominal_values=nominal_values,\n",
    "        motor_initializer=motor_initializer,\n",
    "    ),\n",
    "    # define the random initialisation for load and motor\n",
    "    load=dict(\n",
    "        load_initializer={'random_init': 'uniform', },\n",
    "    ),\n",
    "    reward_function=reward_function,\n",
    "    supply=dict(u_nominal=u_supply),\n",
    "    # define the duration of one sampling step\n",
    "    tau=tau,\n",
    "    callbacks=(reward_logger,),\n",
    "    ode_solver='euler',\n",
    ")\n",
    "\n",
    "env.action_space = Box(low=np.array([-345.0, -345.0, -345.0]), high=np.array([345.0, 345.0, 345.0]), dtype=np.float32)\n",
    "\n",
    "# applying wrappers\n",
    "eps_idx = env.physical_system.state_names.index('epsilon')\n",
    "i_sd_idx = env.physical_system.state_names.index('i_sd')\n",
    "i_sq_idx = env.physical_system.state_names.index('i_sq')\n",
    "env = TimeLimit(\n",
    "    FeatureWrapper(\n",
    "        FlattenObservation(env), \n",
    "        eps_idx, i_sd_idx, i_sq_idx\n",
    "    ),\n",
    "    max_eps_steps\n",
    ")\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efdafe72-ad53-44ed-9faa-7efb8713bdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA ist verfügbar.\n",
      "Aktuelles Gerät: 0\n",
      "Gerätename: NVIDIA GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "buffer_size = 200000 #number of old obsersation steps saved\n",
    "learning_starts = 10000 # memory warmup\n",
    "train_freq = 1 # prediction network gets an update each train_freq's step\n",
    "policy_kwargs = {\n",
    "        'net_arch': [64,64] # hidden layer size of MLP\n",
    "        }\n",
    "exploration_fraction = 0.1 # Fraction of training steps the epsilon decays \n",
    "target_update_interval = 1000 # Target network gets updated each target_update_interval's step\n",
    "verbose = 1 # verbosity of stable-basline's prints\n",
    "simulation_time = 5 # seconds\n",
    "nb_steps = int(simulation_time // tau)\n",
    "\n",
    "# Überprüfen, ob CUDA verfügbar ist\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA ist verfügbar.')\n",
    "    print('Aktuelles Gerät:', torch.cuda.current_device())\n",
    "    print('Gerätename:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print('CUDA ist nicht verfügbar.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df94b630-bb19-4ac3-87c2-8d3be201b637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-28 15:08:16,602] A new study created in memory with name: no-name-0047bda6-08f6-44e8-bf69-2ab0053da835\n",
      "[I 2024-09-28 15:26:42,545] Trial 0 finished with value: -18.989070892333984 and parameters: {'learning_rate': 0.0002504330727589998, 'gamma': 0.9408668832710867, 'batch_size': 256, 'tau': 0.03258165194325543}. Best is trial 0 with value: -18.989070892333984.\n",
      "[I 2024-09-28 15:45:57,666] Trial 1 finished with value: -15.407919883728027 and parameters: {'learning_rate': 0.0006721341215754302, 'gamma': 0.9666994592997383, 'batch_size': 95, 'tau': 0.02590302996728291}. Best is trial 1 with value: -15.407919883728027.\n",
      "[I 2024-09-28 16:05:25,668] Trial 2 finished with value: -18.045833587646484 and parameters: {'learning_rate': 0.0005621844287255635, 'gamma': 0.9726542169760856, 'batch_size': 218, 'tau': 0.011787507436475504}. Best is trial 1 with value: -15.407919883728027.\n",
      "[I 2024-09-28 16:24:44,764] Trial 3 finished with value: -15.394432067871094 and parameters: {'learning_rate': 1.793883540995118e-05, 'gamma': 0.9169848667635434, 'batch_size': 255, 'tau': 0.03065784031088544}. Best is trial 3 with value: -15.394432067871094.\n",
      "[I 2024-09-28 16:43:19,990] Trial 4 finished with value: -23.311344146728516 and parameters: {'learning_rate': 8.141146391578946e-05, 'gamma': 0.9247482146591424, 'batch_size': 200, 'tau': 0.04716166778732267}. Best is trial 3 with value: -15.394432067871094.\n",
      "[I 2024-09-28 17:01:42,639] Trial 5 finished with value: -13.259747505187988 and parameters: {'learning_rate': 0.0001586591411026349, 'gamma': 0.9714700261317379, 'batch_size': 136, 'tau': 0.018950977086955683}. Best is trial 5 with value: -13.259747505187988.\n",
      "[I 2024-09-28 17:27:29,506] Trial 6 finished with value: -12.357501983642578 and parameters: {'learning_rate': 0.00013839421776497046, 'gamma': 0.9482525724197056, 'batch_size': 51, 'tau': 0.01982735288399686}. Best is trial 6 with value: -12.357501983642578.\n",
      "[I 2024-09-28 17:57:51,594] Trial 7 finished with value: -18.73041534423828 and parameters: {'learning_rate': 2.1863763752839225e-05, 'gamma': 0.9980714995379611, 'batch_size': 191, 'tau': 0.020660476706428697}. Best is trial 6 with value: -12.357501983642578.\n",
      "[I 2024-09-28 18:28:34,154] Trial 8 finished with value: -16.70753288269043 and parameters: {'learning_rate': 0.000998128749366671, 'gamma': 0.9086565981800246, 'batch_size': 157, 'tau': 0.013381988723467495}. Best is trial 6 with value: -12.357501983642578.\n",
      "[I 2024-09-28 18:58:55,737] Trial 9 finished with value: -23.36214256286621 and parameters: {'learning_rate': 0.00012464241851570175, 'gamma': 0.9069960886773532, 'batch_size': 140, 'tau': 0.017941187649171794}. Best is trial 6 with value: -12.357501983642578.\n",
      "[I 2024-09-28 19:25:54,162] Trial 10 finished with value: -19.467439651489258 and parameters: {'learning_rate': 6.107888608282716e-05, 'gamma': 0.9445484030888049, 'batch_size': 41, 'tau': 0.003345550118276617}. Best is trial 6 with value: -12.357501983642578.\n",
      "[I 2024-09-28 19:50:52,749] Trial 11 finished with value: -14.849166870117188 and parameters: {'learning_rate': 0.00019364851499266032, 'gamma': 0.9693526776646234, 'batch_size': 42, 'tau': 0.036346897382616206}. Best is trial 6 with value: -12.357501983642578.\n",
      "[I 2024-09-28 20:12:18,910] Trial 12 finished with value: -12.627049446105957 and parameters: {'learning_rate': 3.9647036603659226e-05, 'gamma': 0.9926919932289882, 'batch_size': 92, 'tau': 0.0038177346532604836}. Best is trial 6 with value: -12.357501983642578.\n",
      "[I 2024-09-28 20:32:30,031] Trial 13 finished with value: -17.658342361450195 and parameters: {'learning_rate': 3.997176967556557e-05, 'gamma': 0.9848586379389176, 'batch_size': 79, 'tau': 0.0036308571836895934}. Best is trial 6 with value: -12.357501983642578.\n",
      "[I 2024-09-28 20:59:48,373] Trial 14 finished with value: -20.504610061645508 and parameters: {'learning_rate': 1.091374102543038e-05, 'gamma': 0.9547725440993171, 'batch_size': 86, 'tau': 0.009314199542046044}. Best is trial 6 with value: -12.357501983642578.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Hyperparameter: {'learning_rate': 0.00013839421776497046, 'gamma': 0.9482525724197056, 'batch_size': 51, 'tau': 0.01982735288399686}\n"
     ]
    }
   ],
   "source": [
    "models_dir = \"PMSM_simulation/Tuning/TD3\" #name your directory\n",
    "logdir = \"PMSM_simulation/logs\"\n",
    "total_timesteps=50000\n",
    "\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "# Definiere die Optimierungsfunktion\n",
    "def optimize_td3(trial):\n",
    "    # Zu tunende Hyperparameter\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    gamma = trial.suggest_float('gamma', 0.9, 0.999)\n",
    "    batch_size = trial.suggest_int('batch_size', 32, 256)\n",
    "    tau = trial.suggest_float('tau', 0.001, 0.05)\n",
    "    \n",
    "    model = TD3(\"MlpPolicy\", env, #you can decide between TD3 and DDPG Agent\n",
    "                batch_size=batch_size, gamma=gamma,verbose=0, tensorboard_log=logdir, tau=tau, learning_rate=learning_rate)\n",
    "\n",
    "    reward_logger = RewardLogger()\n",
    "\n",
    "    for i in range(10): \n",
    "        model.learn(total_timesteps=20000, reset_num_timesteps=False, callback=reward_logger, tb_log_name=\"TD3_PMSM__high_ampere_tuning_28.09.24\")\n",
    "        model.save(f\"{models_dir}/model_trial_{trial.number}_steps_{total_timesteps*(i+1)}\")\n",
    "\n",
    "   # Holen der besten Belohnung aus dem RewardLogger\n",
    "    if len(reward_logger.mean_episode_rewards) > 0:\n",
    "        rewards = np.mean(reward_logger.mean_episode_rewards) \n",
    "    else:\n",
    "        rewards = 0.0  # Fallback, falls keine Rewards gesammelt wurden\n",
    "\n",
    "    # Sicherstellen, dass ein gültiger numerischer Wert zurückgegeben wird\n",
    "    if np.any(np.isnan(rewards)):\n",
    "        return 0.0  # Fallback-Wert, falls NaN\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "# Optuna-Studie erstellen und optimieren\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimize_td3, n_trials=15)\n",
    "\n",
    "# Die besten Hyperparameter anzeigen\n",
    "print(\"Beste Hyperparameter:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd3d63-fd98-4fc1-941a-9f2b1bea85f1",
   "metadata": {},
   "source": [
    "#### We now got Tuned Hyperparameters. Let's see if our model improves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c79d3ec-ecea-45d8-9783-1de3c27dfa29",
   "metadata": {},
   "source": [
    "# Training with tuned Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246d340-8382-47ad-9551-bf2de1e79471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use old Rewardlogger class definition or you will get errors\n",
    "models_dir = \"PMSM_simulation/Tuning/TD3_tuned\" #name your directory\n",
    "logdir = \"PMSM_simulation/logs\"\n",
    "total_timesteps=50000\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "\n",
    "learning_rate = 0.00013839421776497046\n",
    "gamma = 0.9482525724197056\n",
    "batch_size = 51\n",
    "tau = 0.01982735288399686\n",
    "buffer_size = 200000 \n",
    "learning_starts = 10000 \n",
    "train_freq = 1 \n",
    "policy_kwargs = {\n",
    "        'net_arch': [64,64] \n",
    "        }\n",
    "exploration_fraction = 0.1 \n",
    "target_update_interval = 1000 \n",
    "verbose = 1\n",
    "simulation_time = 5 # seconds\n",
    "nb_steps = int(simulation_time // tau)\n",
    "    \n",
    "model = TD3(\"MlpPolicy\", env, buffer_size=buffer_size, learning_starts=learning_starts ,train_freq=train_freq, batch_size=batch_size, \n",
    "            gamma=gamma, policy_kwargs=policy_kwargs, verbose=verbose, tau=tau, learning_rate=learning_rate, tensorboard_log=logdir)\n",
    "\n",
    "for i in range(20): \n",
    "    model.learn(total_timesteps=500000, reset_num_timesteps=False, tb_log_name=\"TD3_PMSM__high_ampere_tuned_27.09.24\")\n",
    "    model.save(f\"{models_dir}/model_steps_{total_timesteps*(i+1)}\")\n",
    "\n",
    "env.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b696f1d3-0d34-44cd-a6a9-5241b3d83c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
